{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](./img/TheBridge_RL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Autónomo (Smartcab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Objetivo](#Objetivo)  \n",
    "* [Recompensas (Rewards)](#Recompensas-(Rewards))  \n",
    "* [Espacio de Estados (State Space)](#Espacio-de-Estados-(State-Space))  \n",
    "* [Action space](#Action-space)  \n",
    "* [Implementacion](#Implementacion)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "  \n",
    "[al indice](#Contenidos)  \n",
    "\n",
    "El trabajo del Smartcab es recoger al pasajero en un lugar y dejarlo en otro. Algunos detalles que nos encantaría que nuestro Smartcab tenga en cuenta serían:\n",
    "\n",
    "* Dejar al pasajero en la ubicación correcta.  \n",
    "* Ahorrar tiempo al pasajero dedicando el mínimo tiempo posible para dejarlo.  \n",
    "* Cuidar la seguridad del pasajero y respetar las normas de tráfico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensas (Rewards)  \n",
    "[al indice](#Contenidos)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El agente debería recibir una alta recompensa positiva por una entrega del cliente exitosa porque este comportamiento es de los más importantes que queremos que aprenda.\n",
    "* El agente debería ser penalizado si intenta dejar a un pasajero en destinos incorrectos.\n",
    "* El agente debería recibir una ligera recompensa negativa por no llegar a destino después de cada intervalo de tiempo. \"Ligera\" negativa porque preferiríamos que nuestro agente llegue tarde en lugar de hacer movimientos erróneos tratando de llegar al destino lo más rápido posible.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro pequeño ejercicio vamos a establecer las siguientes \"recompensas\":\n",
    "* Recibimos +20 puntos por un traslado exitoso.  \n",
    "* Perdemos 1 punto por cada intervalo de tiempo que tarda.  \n",
    "* También hay una penalización de 10 puntos por acciones de recogida y dejada ilegales. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de Estados (State Space)  \n",
    "[al indice](#Contenidos)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El __Espacio de Estados__ es el conjunto de todas las posibles situaciones en las que nuestro taxi podría estar. El estado además debe contener información útil que el agente necesite para tomar la acción correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement_Learning_Taxi_Env.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que Smartcab es el único vehículo en este circuito de aprendizaje. Nuestro circuito está dividido en __una cuadrícula de 5x5__, lo que nos da __25 posibles ubicaciones__ para el taxi (posiciones (0,0) a (5,5)). Estas 25 ubicaciones __son una parte de nuestro espacio de estados__. Observa que la __ubicación actual__ de nuestro taxi es la __coordenada (3, 1)__.\n",
    "\n",
    "También puedes ver que hay __cuatro (4) ubicaciones__ en las que podemos __recoger y dejar__ a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en coordenadas (fila, columna).  \n",
    "\n",
    "También debemos tener en cuenta un (1) estado adicional del pasajero de estar dentro del taxi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿cómo codificaríamos el estado anterior de la forma más compacta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3,1,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta lo anterior, ¿cuántos estados posibles hay en nuestro State Space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*5*5*4 #5 filas x 5 columnas x 5 estados del pasajero (4 posiciones de recogida + el taxi) y 4 posibles valores de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entorno nos devolverá un índice entre 0 y 500 para representar el estado al que se llega después de ejecutar una acción (invocando el método step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space  \n",
    "[al indice](#Contenidos)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente se encuentra con uno de los 500 estados y toma una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero.\n",
    "\n",
    "En otras palabras, tenemos seis posibles acciones:\n",
    "\n",
    "0. sur\n",
    "1. norte\n",
    "2. este\n",
    "3. oeste\n",
    "4. recoger\n",
    "5. dejar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según la ilustración, el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes (por ejemplo mover de la posición (3,1) a la (3,0)).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./img/Reinforcement_Learning_Taxi_Env.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entorno proporciona una penalización de -1 por cada movimiento no permitido y el taxi no se moverá a ningún lado. Como siguiente estado devuelve el mismo de partida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion  \n",
    "[al indice](#Contenidos)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env #saltarnos la limitación de 200 pasos que viene por defecto en gym\n",
    "\n",
    "state,info = env.reset(seed = 19)\n",
    "\n",
    "print(env.render())\n",
    "#plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El cuadrado relleno representa el taxi, que es de color amarillo sin un pasajero y verde con un pasajero. En este caso, comenzaríamos con nuestro taxi situado en (2,0).\n",
    "* La barra (\"|\") representa una pared que el taxi no puede cruzar.\n",
    "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida del pasajero, y la letra morada es el destino actual. Es decir hay que recogerlo de Y y entregarlo en R, para este ejemplo generado al resetear el entorno con la semilla ajustada al valor 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial: 208\n"
     ]
    }
   ],
   "source": [
    "print(\"Estado inicial:\",state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieramos camibar la posición de partida de nuestro taxi y llevarlo a la de la figura, podemos moverlo con las acciones e ignorar las recompensas. Bastaría con moverlo al este y luego al sur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movements = [2,0]\n",
    "for action in movements:\n",
    "    env.step(action)\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
